{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29f863c7-52e9-4f51-ae00-bc3c97f2ba53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nincase the above code does not work, you can use the absolute path instead\\nsys.path.append(r\".\")\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import os \n",
    "sys.path.append(\"/home/zichong/virtual_cell_challenge/wgr/WGR\") \n",
    "\n",
    "\"\"\"\n",
    "incase the above code does not work, you can use the absolute path instead\n",
    "sys.path.append(r\".\\\")\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09f578f4-d40c-4e9c-b234-ea17249bff03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ec819f9-bbc8-4b4f-bb26-cf48a2ce0333",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.basic_utils import setup_seed\n",
    "from data.SimulationData import DataGenerator \n",
    "from utils.training_utils import train_WGR_fnn\n",
    "from utils.evaluation_utils import L1L2_MSE_mean_sd_G, MSE_quantile_G_uniY\n",
    "from models.generator import generator_fnn\n",
    "from models.discriminator import discriminator_fnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82f422a-70a8-4191-b99d-16612d3dbef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(Xdim=5, Ydim=1, model='M1', noise_dim=5, noise_dist='gaussian', train=5000, val=1000, test=1000, train_batch=128, val_batch=100, test_batch=100, epochs=100, reps=100)\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "\n",
    "if 'ipykernel_launcher.py' in sys.argv[0]:  #if not work in jupyter, you can delete this part\n",
    "    import sys\n",
    "    sys.argv = [sys.argv[0]] \n",
    "\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Implementation of WGR for M1')\n",
    "\n",
    "parser.add_argument('--Xdim', default=5, type=int, help='dimensionality of X')\n",
    "parser.add_argument('--Ydim', default=1, type=int, help='dimensionality of Y')\n",
    "parser.add_argument('--model', default='M1', type=str, help='model')\n",
    "\n",
    "parser.add_argument('--noise_dim', default=5, type=int, help='dimensionality of noise vector')\n",
    "parser.add_argument('--noise_dist', default='gaussian', type=str, help='distribution of noise vector')\n",
    "\n",
    "parser.add_argument('--train', default=5000, type=int, help='size of train dataset')\n",
    "parser.add_argument('--val', default=1000, type=int, help='size of validation dataset')\n",
    "parser.add_argument('--test', default=1000, type=int, help='size of test dataset')\n",
    "\n",
    "parser.add_argument('--train_batch', default=128, type=int, metavar='BS', help='batch size while training')\n",
    "parser.add_argument('--val_batch', default=100, type=int, metavar='BS', help='batch size while validation')\n",
    "parser.add_argument('--test_batch', default=100, type=int, metavar='BS', help='batch size while testing')\n",
    "parser.add_argument('--epochs', default=100, type=int, help='number of epochs to train')\n",
    "parser.add_argument('--reps', default=100, type=int, help='number of replications')\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1c1b44a-0c6c-478a-b807-e5e1da20a1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed \n",
    "setup_seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "177a8bd5-74a6-4ffe-ab47-e2e7cc821c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data from M1\n",
    "data_gen = DataGenerator(args)\n",
    "DATA = data_gen.generate_data('M1')\n",
    "train_X, train_Y = DATA['train_X'], DATA['train_Y']\n",
    "val_X, val_Y = DATA['val_X'], DATA['val_Y']\n",
    "test_X, test_Y = DATA['test_X'], DATA['test_Y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de288fbd-c582-4ad8-b10d-59b3d145e2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TensorDatasets and initialize a DataLoaders\n",
    "train_dataset = TensorDataset( train_X.float(), train_Y.float() )\n",
    "loader_train = DataLoader(train_dataset , batch_size=args.train_batch, shuffle=True)\n",
    "\n",
    "val_dataset = TensorDataset( val_X.float(), val_Y.float() )\n",
    "loader_val = DataLoader(val_dataset , batch_size=args.val_batch, shuffle=True)\n",
    "\n",
    "test_dataset = TensorDataset( test_X.float(), test_Y.float() )\n",
    "loader_test  = DataLoader(test_dataset , batch_size=args.test_batch, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20bb4d11-47ef-4e17-9852-27ea723f271c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=6, out_features=64, bias=True)\n",
       "  (1): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "  (2): Linear(in_features=64, out_features=32, bias=True)\n",
       "  (3): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "  (4): Linear(in_features=32, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define generator network and discriminator network\n",
    "G_net = generator_fnn(Xdim=args.Xdim, Ydim=args.Ydim, noise_dim=args.noise_dim, hidden_dims = [64, 32])\n",
    "D_net = discriminator_fnn(input_dim=args.Xdim+args.Ydim, hidden_dims = [64, 32])\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "G_net.to(device)\n",
    "D_net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08ccda7e-1d68-4ff1-9e4b-d5caa66dcf81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize RMSprop optimizers\n",
    "D_solver = optim.RMSprop(D_net.parameters(),lr = 0.001)\n",
    "G_solver = optim.RMSprop(G_net.parameters(),lr = 0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c643e20e-1daf-420e-9145-fac2f3c39e32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean L1 Loss: 3.427226, Mean L2 Loss: 23.734234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zichong/miniconda3/envs/torch/lib/python3.12/site-packages/torch/autograd/graph.py:829: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:179.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - D Loss: 0.8459, G Loss: 1.2085\n",
      "Epoch 1 - D Loss: 0.1244, G Loss: 0.8511\n",
      "Epoch 2 - D Loss: 0.0902, G Loss: 0.9278\n",
      "Epoch 3 - D Loss: 0.0463, G Loss: 1.2549\n",
      "Epoch 4 - D Loss: 0.0345, G Loss: 1.3455\n",
      "Epoch 5 - D Loss: 0.0329, G Loss: 1.2731\n",
      "Epoch 6 - D Loss: 0.0283, G Loss: 1.1749\n",
      "Epoch 7 - D Loss: 0.0194, G Loss: 1.1224\n",
      "Epoch 8 - D Loss: 0.0097, G Loss: 1.0897\n",
      "Epoch 9 - D Loss: 0.0144, G Loss: 1.0039\n",
      "Epoch 10 - D Loss: -0.0037, G Loss: 0.9916\n",
      "Epoch 11 - D Loss: -0.0009, G Loss: 0.9552\n",
      "Epoch 12 - D Loss: 0.0033, G Loss: 0.9232\n",
      "Epoch 13 - D Loss: 0.0003, G Loss: 0.8426\n",
      "Epoch 14 - D Loss: 0.0071, G Loss: 0.8498\n",
      "Epoch 15 - D Loss: 0.0054, G Loss: 0.8801\n",
      "Epoch 16 - D Loss: 0.0064, G Loss: 0.8987\n",
      "Epoch 17 - D Loss: 0.0129, G Loss: 0.8659\n",
      "Epoch 18 - D Loss: 0.0069, G Loss: 0.9387\n",
      "Epoch 19 - D Loss: 0.0013, G Loss: 0.9049\n",
      "Epoch 20 - D Loss: 0.0051, G Loss: 1.0355\n",
      "Epoch 21 - D Loss: 0.0026, G Loss: 0.9628\n",
      "Epoch 22 - D Loss: 0.0015, G Loss: 1.0376\n",
      "Epoch 23 - D Loss: -0.0017, G Loss: 1.0247\n",
      "Epoch 24 - D Loss: -0.0044, G Loss: 1.0723\n",
      "Mean L1 Loss: 1.267813, Mean L2 Loss: 3.634192\n",
      "Epoch 25, Iter 1000, D Loss: -0.0065, G Loss: 1.1000, L1: 1.2678, L2: 3.6342\n",
      "Saved best model with L2: 3.6342\n",
      "Epoch 25 - D Loss: -0.0074, G Loss: 1.0926\n",
      "Mean L1 Loss: 1.255009, Mean L2 Loss: 3.494324\n",
      "Epoch 26, Iter 1050, D Loss: -0.0060, G Loss: 1.1083, L1: 1.2550, L2: 3.4943\n",
      "Saved best model with L2: 3.4943\n",
      "Epoch 26 - D Loss: -0.0056, G Loss: 1.1057\n",
      "Epoch 27 - D Loss: -0.0049, G Loss: 1.1701\n",
      "Mean L1 Loss: 1.256657, Mean L2 Loss: 3.521382\n",
      "Epoch 28, Iter 1100, D Loss: 0.0080, G Loss: 1.1675, L1: 1.2567, L2: 3.5214\n",
      "Epoch 28 - D Loss: -0.0104, G Loss: 1.1854\n",
      "Mean L1 Loss: 1.267424, Mean L2 Loss: 3.589814\n",
      "Epoch 29, Iter 1150, D Loss: -0.0164, G Loss: 1.1779, L1: 1.2674, L2: 3.5898\n",
      "Epoch 29 - D Loss: -0.0100, G Loss: 1.1645\n",
      "Mean L1 Loss: 1.257315, Mean L2 Loss: 3.521832\n",
      "Epoch 30, Iter 1200, D Loss: -0.0157, G Loss: 1.2579, L1: 1.2573, L2: 3.5218\n",
      "Epoch 30 - D Loss: -0.0143, G Loss: 1.2614\n",
      "Epoch 31 - D Loss: -0.0098, G Loss: 1.1675\n",
      "Mean L1 Loss: 1.255375, Mean L2 Loss: 3.478781\n",
      "Epoch 32, Iter 1250, D Loss: -0.0463, G Loss: 1.3651, L1: 1.2554, L2: 3.4788\n",
      "Saved best model with L2: 3.4788\n",
      "Epoch 32 - D Loss: -0.0101, G Loss: 1.2447\n",
      "Mean L1 Loss: 1.234065, Mean L2 Loss: 3.399863\n",
      "Epoch 33, Iter 1300, D Loss: -0.0047, G Loss: 1.1209, L1: 1.2341, L2: 3.3999\n",
      "Saved best model with L2: 3.3999\n",
      "Epoch 33 - D Loss: -0.0084, G Loss: 1.1842\n",
      "Mean L1 Loss: 1.233626, Mean L2 Loss: 3.385159\n",
      "Epoch 34, Iter 1350, D Loss: -0.0188, G Loss: 1.2461, L1: 1.2336, L2: 3.3852\n",
      "Saved best model with L2: 3.3852\n",
      "Epoch 34 - D Loss: -0.0158, G Loss: 1.2316\n",
      "Mean L1 Loss: 1.244071, Mean L2 Loss: 3.423421\n",
      "Epoch 35, Iter 1400, D Loss: -0.0066, G Loss: 1.2453, L1: 1.2441, L2: 3.4234\n",
      "Epoch 35 - D Loss: -0.0079, G Loss: 1.2480\n",
      "Epoch 36 - D Loss: -0.0124, G Loss: 1.2291\n",
      "Mean L1 Loss: 1.257575, Mean L2 Loss: 3.448927\n",
      "Epoch 37, Iter 1450, D Loss: -0.0034, G Loss: 1.1507, L1: 1.2576, L2: 3.4489\n",
      "Epoch 37 - D Loss: -0.0145, G Loss: 1.1942\n",
      "Mean L1 Loss: 1.247349, Mean L2 Loss: 3.375066\n",
      "Epoch 38, Iter 1500, D Loss: -0.0127, G Loss: 1.2096, L1: 1.2473, L2: 3.3751\n",
      "Saved best model with L2: 3.3751\n",
      "Epoch 38 - D Loss: -0.0137, G Loss: 1.1839\n",
      "Mean L1 Loss: 1.246574, Mean L2 Loss: 3.382189\n",
      "Epoch 39, Iter 1550, D Loss: -0.0059, G Loss: 1.1899, L1: 1.2466, L2: 3.3822\n",
      "Epoch 39 - D Loss: -0.0096, G Loss: 1.1941\n",
      "Epoch 40 - D Loss: -0.0171, G Loss: 1.2194\n",
      "Mean L1 Loss: 1.237478, Mean L2 Loss: 3.311999\n",
      "Epoch 41, Iter 1600, D Loss: -0.0720, G Loss: 1.2201, L1: 1.2375, L2: 3.3120\n",
      "Saved best model with L2: 3.3120\n",
      "Epoch 41 - D Loss: -0.0142, G Loss: 1.1963\n",
      "Mean L1 Loss: 1.242215, Mean L2 Loss: 3.360811\n",
      "Epoch 42, Iter 1650, D Loss: -0.0138, G Loss: 1.1107, L1: 1.2422, L2: 3.3608\n",
      "Epoch 42 - D Loss: -0.0081, G Loss: 1.1999\n",
      "Mean L1 Loss: 1.255665, Mean L2 Loss: 3.385141\n",
      "Epoch 43, Iter 1700, D Loss: -0.0085, G Loss: 1.2176, L1: 1.2557, L2: 3.3851\n",
      "Epoch 43 - D Loss: -0.0074, G Loss: 1.1832\n",
      "Mean L1 Loss: 1.229194, Mean L2 Loss: 3.288188\n",
      "Epoch 44, Iter 1750, D Loss: -0.0079, G Loss: 1.1920, L1: 1.2292, L2: 3.2882\n",
      "Saved best model with L2: 3.2882\n",
      "Epoch 44 - D Loss: -0.0091, G Loss: 1.1966\n",
      "Epoch 45 - D Loss: -0.0066, G Loss: 1.2217\n",
      "Mean L1 Loss: 1.228031, Mean L2 Loss: 3.300661\n",
      "Epoch 46, Iter 1800, D Loss: -0.0118, G Loss: 1.1099, L1: 1.2280, L2: 3.3007\n",
      "Epoch 46 - D Loss: -0.0040, G Loss: 1.2001\n",
      "Mean L1 Loss: 1.252753, Mean L2 Loss: 3.319610\n",
      "Epoch 47, Iter 1850, D Loss: -0.0089, G Loss: 1.2024, L1: 1.2528, L2: 3.3196\n",
      "Epoch 47 - D Loss: -0.0079, G Loss: 1.2094\n",
      "Mean L1 Loss: 1.246638, Mean L2 Loss: 3.290573\n",
      "Epoch 48, Iter 1900, D Loss: 0.0034, G Loss: 1.2356, L1: 1.2466, L2: 3.2906\n",
      "Epoch 48 - D Loss: -0.0009, G Loss: 1.2306\n",
      "Mean L1 Loss: 1.230859, Mean L2 Loss: 3.268236\n",
      "Epoch 49, Iter 1950, D Loss: -0.0035, G Loss: 1.2012, L1: 1.2309, L2: 3.2682\n",
      "Saved best model with L2: 3.2682\n",
      "Epoch 49 - D Loss: -0.0035, G Loss: 1.2012\n",
      "Epoch 50 - D Loss: 0.0011, G Loss: 1.1746\n",
      "Mean L1 Loss: 1.235020, Mean L2 Loss: 3.250095\n",
      "Epoch 51, Iter 2000, D Loss: -0.0162, G Loss: 1.1446, L1: 1.2350, L2: 3.2501\n",
      "Saved best model with L2: 3.2501\n",
      "Epoch 51 - D Loss: -0.0032, G Loss: 1.2154\n",
      "Mean L1 Loss: 1.221877, Mean L2 Loss: 3.232010\n",
      "Epoch 52, Iter 2050, D Loss: 0.0045, G Loss: 1.2401, L1: 1.2219, L2: 3.2320\n",
      "Saved best model with L2: 3.2320\n",
      "Epoch 52 - D Loss: 0.0033, G Loss: 1.2220\n",
      "Mean L1 Loss: 1.218290, Mean L2 Loss: 3.178105\n",
      "Epoch 53, Iter 2100, D Loss: 0.0094, G Loss: 1.2143, L1: 1.2183, L2: 3.1781\n",
      "Saved best model with L2: 3.1781\n",
      "Epoch 53 - D Loss: 0.0074, G Loss: 1.2228\n",
      "Epoch 54 - D Loss: 0.0026, G Loss: 1.2760\n",
      "Mean L1 Loss: 1.233849, Mean L2 Loss: 3.266387\n",
      "Epoch 55, Iter 2150, D Loss: 0.0092, G Loss: 1.2108, L1: 1.2338, L2: 3.2664\n",
      "Epoch 55 - D Loss: 0.0034, G Loss: 1.2303\n",
      "Mean L1 Loss: 1.228141, Mean L2 Loss: 3.263662\n",
      "Epoch 56, Iter 2200, D Loss: 0.0032, G Loss: 1.2617, L1: 1.2281, L2: 3.2637\n",
      "Epoch 56 - D Loss: -0.0002, G Loss: 1.2431\n",
      "Mean L1 Loss: 1.232264, Mean L2 Loss: 3.248714\n",
      "Epoch 57, Iter 2250, D Loss: 0.0006, G Loss: 1.2423, L1: 1.2323, L2: 3.2487\n",
      "Epoch 57 - D Loss: 0.0049, G Loss: 1.2618\n",
      "Mean L1 Loss: 1.256354, Mean L2 Loss: 3.334423\n",
      "Epoch 58, Iter 2300, D Loss: 0.0083, G Loss: 1.2886, L1: 1.2564, L2: 3.3344\n",
      "Epoch 58 - D Loss: 0.0088, G Loss: 1.2895\n",
      "Epoch 59 - D Loss: 0.0022, G Loss: 1.2735\n",
      "Mean L1 Loss: 1.260071, Mean L2 Loss: 3.332074\n",
      "Epoch 60, Iter 2350, D Loss: 0.0153, G Loss: 1.2497, L1: 1.2601, L2: 3.3321\n",
      "Epoch 60 - D Loss: 0.0014, G Loss: 1.2998\n",
      "Mean L1 Loss: 1.264892, Mean L2 Loss: 3.353003\n",
      "Epoch 61, Iter 2400, D Loss: -0.0014, G Loss: 1.4665, L1: 1.2649, L2: 3.3530\n",
      "Epoch 61 - D Loss: 0.0013, G Loss: 1.4086\n",
      "Mean L1 Loss: 1.221132, Mean L2 Loss: 3.169613\n",
      "Epoch 62, Iter 2450, D Loss: 0.0086, G Loss: 1.3754, L1: 1.2211, L2: 3.1696\n",
      "Saved best model with L2: 3.1696\n",
      "Epoch 62 - D Loss: 0.0059, G Loss: 1.3826\n",
      "Epoch 63 - D Loss: -0.0009, G Loss: 1.3278\n",
      "Mean L1 Loss: 1.223703, Mean L2 Loss: 3.183871\n",
      "Epoch 64, Iter 2500, D Loss: 0.0006, G Loss: 1.2721, L1: 1.2237, L2: 3.1839\n",
      "Epoch 64 - D Loss: 0.0040, G Loss: 1.3756\n",
      "Mean L1 Loss: 1.238676, Mean L2 Loss: 3.300482\n",
      "Epoch 65, Iter 2550, D Loss: -0.0050, G Loss: 1.3097, L1: 1.2387, L2: 3.3005\n",
      "Epoch 65 - D Loss: 0.0012, G Loss: 1.3754\n",
      "Mean L1 Loss: 1.229071, Mean L2 Loss: 3.203278\n",
      "Epoch 66, Iter 2600, D Loss: 0.0038, G Loss: 1.4610, L1: 1.2291, L2: 3.2033\n",
      "Epoch 66 - D Loss: 0.0027, G Loss: 1.4287\n",
      "Mean L1 Loss: 1.229074, Mean L2 Loss: 3.188238\n",
      "Epoch 67, Iter 2650, D Loss: 0.0036, G Loss: 1.4346, L1: 1.2291, L2: 3.1882\n",
      "Epoch 67 - D Loss: 0.0039, G Loss: 1.4306\n",
      "Epoch 68 - D Loss: -0.0030, G Loss: 1.4403\n",
      "Mean L1 Loss: 1.227890, Mean L2 Loss: 3.212371\n",
      "Epoch 69, Iter 2700, D Loss: -0.0045, G Loss: 1.4973, L1: 1.2279, L2: 3.2124\n",
      "Epoch 69 - D Loss: 0.0004, G Loss: 1.4981\n",
      "Mean L1 Loss: 1.225377, Mean L2 Loss: 3.228963\n",
      "Epoch 70, Iter 2750, D Loss: -0.0007, G Loss: 1.4788, L1: 1.2254, L2: 3.2290\n",
      "Epoch 70 - D Loss: -0.0000, G Loss: 1.4663\n",
      "Mean L1 Loss: 1.223817, Mean L2 Loss: 3.196577\n",
      "Epoch 71, Iter 2800, D Loss: 0.0098, G Loss: 1.4552, L1: 1.2238, L2: 3.1966\n",
      "Epoch 71 - D Loss: 0.0067, G Loss: 1.4671\n",
      "Epoch 72 - D Loss: 0.0017, G Loss: 1.4812\n",
      "Mean L1 Loss: 1.256306, Mean L2 Loss: 3.295478\n",
      "Epoch 73, Iter 2850, D Loss: -0.0020, G Loss: 1.4604, L1: 1.2563, L2: 3.2955\n",
      "Epoch 73 - D Loss: -0.0004, G Loss: 1.4942\n",
      "Mean L1 Loss: 1.221990, Mean L2 Loss: 3.199427\n",
      "Epoch 74, Iter 2900, D Loss: 0.0157, G Loss: 1.4717, L1: 1.2220, L2: 3.1994\n",
      "Epoch 74 - D Loss: 0.0073, G Loss: 1.4922\n",
      "Mean L1 Loss: 1.276696, Mean L2 Loss: 3.337689\n",
      "Epoch 75, Iter 2950, D Loss: 0.0035, G Loss: 1.4989, L1: 1.2767, L2: 3.3377\n",
      "Epoch 75 - D Loss: 0.0010, G Loss: 1.5294\n",
      "Mean L1 Loss: 1.233980, Mean L2 Loss: 3.235757\n",
      "Epoch 76, Iter 3000, D Loss: 0.0000, G Loss: 1.5337, L1: 1.2340, L2: 3.2358\n",
      "Epoch 76 - D Loss: 0.0011, G Loss: 1.5348\n",
      "Epoch 77 - D Loss: 0.0021, G Loss: 1.4872\n",
      "Mean L1 Loss: 1.230277, Mean L2 Loss: 3.295336\n",
      "Epoch 78, Iter 3050, D Loss: 0.0142, G Loss: 1.4515, L1: 1.2303, L2: 3.2953\n",
      "Epoch 78 - D Loss: 0.0025, G Loss: 1.5308\n",
      "Mean L1 Loss: 1.245372, Mean L2 Loss: 3.292725\n",
      "Epoch 79, Iter 3100, D Loss: 0.0001, G Loss: 1.5874, L1: 1.2454, L2: 3.2927\n",
      "Epoch 79 - D Loss: 0.0027, G Loss: 1.5585\n",
      "Mean L1 Loss: 1.219177, Mean L2 Loss: 3.185159\n",
      "Epoch 80, Iter 3150, D Loss: -0.0016, G Loss: 1.5521, L1: 1.2192, L2: 3.1852\n",
      "Epoch 80 - D Loss: -0.0026, G Loss: 1.5685\n",
      "Epoch 81 - D Loss: -0.0009, G Loss: 1.5805\n",
      "Mean L1 Loss: 1.207943, Mean L2 Loss: 3.164268\n",
      "Epoch 82, Iter 3200, D Loss: 0.0056, G Loss: 1.4812, L1: 1.2079, L2: 3.1643\n",
      "Saved best model with L2: 3.1643\n",
      "Epoch 82 - D Loss: -0.0004, G Loss: 1.5655\n",
      "Mean L1 Loss: 1.235368, Mean L2 Loss: 3.230851\n",
      "Epoch 83, Iter 3250, D Loss: 0.0033, G Loss: 1.5375, L1: 1.2354, L2: 3.2309\n",
      "Epoch 83 - D Loss: -0.0013, G Loss: 1.5974\n",
      "Mean L1 Loss: 1.216953, Mean L2 Loss: 3.162102\n",
      "Epoch 84, Iter 3300, D Loss: 0.0016, G Loss: 1.5638, L1: 1.2170, L2: 3.1621\n",
      "Saved best model with L2: 3.1621\n",
      "Epoch 84 - D Loss: -0.0004, G Loss: 1.5768\n",
      "Mean L1 Loss: 1.219018, Mean L2 Loss: 3.240853\n",
      "Epoch 85, Iter 3350, D Loss: -0.0010, G Loss: 1.6327, L1: 1.2190, L2: 3.2409\n",
      "Epoch 85 - D Loss: -0.0002, G Loss: 1.6254\n",
      "Epoch 86 - D Loss: 0.0030, G Loss: 1.5809\n",
      "Mean L1 Loss: 1.222047, Mean L2 Loss: 3.164504\n",
      "Epoch 87, Iter 3400, D Loss: -0.0109, G Loss: 1.5843, L1: 1.2220, L2: 3.1645\n",
      "Epoch 87 - D Loss: -0.0059, G Loss: 1.6154\n",
      "Mean L1 Loss: 1.228440, Mean L2 Loss: 3.256435\n",
      "Epoch 88, Iter 3450, D Loss: 0.0038, G Loss: 1.6301, L1: 1.2284, L2: 3.2564\n",
      "Epoch 88 - D Loss: -0.0012, G Loss: 1.6477\n",
      "Mean L1 Loss: 1.217074, Mean L2 Loss: 3.188839\n",
      "Epoch 89, Iter 3500, D Loss: -0.0034, G Loss: 1.5488, L1: 1.2171, L2: 3.1888\n",
      "Epoch 89 - D Loss: -0.0003, G Loss: 1.5709\n",
      "Epoch 90 - D Loss: 0.0020, G Loss: 1.6047\n",
      "Mean L1 Loss: 1.216650, Mean L2 Loss: 3.179673\n",
      "Epoch 91, Iter 3550, D Loss: -0.0526, G Loss: 1.7178, L1: 1.2167, L2: 3.1797\n",
      "Epoch 91 - D Loss: 0.0005, G Loss: 1.5965\n",
      "Mean L1 Loss: 1.232099, Mean L2 Loss: 3.233166\n",
      "Epoch 92, Iter 3600, D Loss: -0.0007, G Loss: 1.6076, L1: 1.2321, L2: 3.2332\n",
      "Epoch 92 - D Loss: -0.0049, G Loss: 1.6440\n",
      "Mean L1 Loss: 1.208279, Mean L2 Loss: 3.145918\n",
      "Epoch 93, Iter 3650, D Loss: -0.0033, G Loss: 1.6377, L1: 1.2083, L2: 3.1459\n",
      "Saved best model with L2: 3.1459\n",
      "Epoch 93 - D Loss: -0.0011, G Loss: 1.6046\n",
      "Mean L1 Loss: 1.218515, Mean L2 Loss: 3.168965\n",
      "Epoch 94, Iter 3700, D Loss: -0.0035, G Loss: 1.6504, L1: 1.2185, L2: 3.1690\n",
      "Epoch 94 - D Loss: -0.0024, G Loss: 1.6371\n",
      "Epoch 95 - D Loss: -0.0004, G Loss: 1.5653\n",
      "Mean L1 Loss: 1.227255, Mean L2 Loss: 3.326349\n",
      "Epoch 96, Iter 3750, D Loss: -0.0044, G Loss: 1.6140, L1: 1.2273, L2: 3.3263\n",
      "Epoch 96 - D Loss: 0.0017, G Loss: 1.6061\n",
      "Mean L1 Loss: 1.229371, Mean L2 Loss: 3.267480\n",
      "Epoch 97, Iter 3800, D Loss: -0.0012, G Loss: 1.5730, L1: 1.2294, L2: 3.2675\n",
      "Epoch 97 - D Loss: -0.0002, G Loss: 1.5643\n",
      "Mean L1 Loss: 1.206664, Mean L2 Loss: 3.141371\n",
      "Epoch 98, Iter 3850, D Loss: -0.0013, G Loss: 1.5912, L1: 1.2067, L2: 3.1414\n",
      "Saved best model with L2: 3.1414\n",
      "Epoch 98 - D Loss: -0.0077, G Loss: 1.5898\n",
      "Mean L1 Loss: 1.217860, Mean L2 Loss: 3.170686\n",
      "Epoch 99, Iter 3900, D Loss: 0.0012, G Loss: 1.6046, L1: 1.2179, L2: 3.1707\n",
      "Epoch 99 - D Loss: 0.0012, G Loss: 1.6046\n",
      "Epoch 100 - D Loss: 0.0010, G Loss: 1.5845\n",
      "Mean L1 Loss: 1.225658, Mean L2 Loss: 3.194821\n",
      "Epoch 101, Iter 3950, D Loss: 0.0023, G Loss: 1.6270, L1: 1.2257, L2: 3.1948\n",
      "Epoch 101 - D Loss: -0.0021, G Loss: 1.6030\n",
      "Mean L1 Loss: 1.223511, Mean L2 Loss: 3.212562\n",
      "Epoch 102, Iter 4000, D Loss: -0.0036, G Loss: 1.5629, L1: 1.2235, L2: 3.2126\n",
      "Epoch 102 - D Loss: -0.0019, G Loss: 1.6029\n",
      "Mean L1 Loss: 1.229872, Mean L2 Loss: 3.244766\n",
      "Epoch 103, Iter 4050, D Loss: -0.0019, G Loss: 1.5706, L1: 1.2299, L2: 3.2448\n",
      "Epoch 103 - D Loss: 0.0016, G Loss: 1.5799\n",
      "Epoch 104 - D Loss: -0.0008, G Loss: 1.5706\n",
      "Mean L1 Loss: 1.257089, Mean L2 Loss: 3.244732\n",
      "Epoch 105, Iter 4100, D Loss: -0.0041, G Loss: 1.5435, L1: 1.2571, L2: 3.2447\n",
      "Epoch 105 - D Loss: 0.0027, G Loss: 1.5575\n",
      "Mean L1 Loss: 1.216169, Mean L2 Loss: 3.184244\n",
      "Epoch 106, Iter 4150, D Loss: 0.0002, G Loss: 1.5608, L1: 1.2162, L2: 3.1842\n",
      "Epoch 106 - D Loss: -0.0020, G Loss: 1.5804\n",
      "Mean L1 Loss: 1.217230, Mean L2 Loss: 3.225292\n",
      "Epoch 107, Iter 4200, D Loss: 0.0001, G Loss: 1.5612, L1: 1.2172, L2: 3.2253\n",
      "Epoch 107 - D Loss: 0.0007, G Loss: 1.5775\n",
      "Mean L1 Loss: 1.206306, Mean L2 Loss: 3.142235\n",
      "Epoch 108, Iter 4250, D Loss: -0.0014, G Loss: 1.5715, L1: 1.2063, L2: 3.1422\n",
      "Epoch 108 - D Loss: -0.0014, G Loss: 1.5701\n",
      "Epoch 109 - D Loss: 0.0007, G Loss: 1.5805\n",
      "Mean L1 Loss: 1.231100, Mean L2 Loss: 3.232301\n",
      "Epoch 110, Iter 4300, D Loss: -0.0043, G Loss: 1.6256, L1: 1.2311, L2: 3.2323\n",
      "Epoch 110 - D Loss: -0.0032, G Loss: 1.6027\n",
      "Mean L1 Loss: 1.214278, Mean L2 Loss: 3.200385\n",
      "Epoch 111, Iter 4350, D Loss: -0.0021, G Loss: 1.6046, L1: 1.2143, L2: 3.2004\n",
      "Epoch 111 - D Loss: 0.0000, G Loss: 1.6167\n",
      "Mean L1 Loss: 1.230845, Mean L2 Loss: 3.218932\n",
      "Epoch 112, Iter 4400, D Loss: 0.0030, G Loss: 1.6013, L1: 1.2308, L2: 3.2189\n",
      "Epoch 112 - D Loss: 0.0025, G Loss: 1.5887\n",
      "Epoch 113 - D Loss: -0.0020, G Loss: 1.6136\n",
      "Mean L1 Loss: 1.251964, Mean L2 Loss: 3.350552\n",
      "Epoch 114, Iter 4450, D Loss: 0.0020, G Loss: 1.5554, L1: 1.2520, L2: 3.3506\n",
      "Epoch 114 - D Loss: 0.0036, G Loss: 1.6341\n",
      "Mean L1 Loss: 1.221702, Mean L2 Loss: 3.143825\n",
      "Epoch 115, Iter 4500, D Loss: 0.0021, G Loss: 1.5578, L1: 1.2217, L2: 3.1438\n",
      "Epoch 115 - D Loss: -0.0035, G Loss: 1.5507\n",
      "Mean L1 Loss: 1.235977, Mean L2 Loss: 3.244449\n",
      "Epoch 116, Iter 4550, D Loss: 0.0060, G Loss: 1.5517, L1: 1.2360, L2: 3.2444\n",
      "Epoch 116 - D Loss: 0.0032, G Loss: 1.5800\n",
      "Mean L1 Loss: 1.262224, Mean L2 Loss: 3.359409\n",
      "Epoch 117, Iter 4600, D Loss: -0.0019, G Loss: 1.6019, L1: 1.2622, L2: 3.3594\n",
      "Epoch 117 - D Loss: -0.0033, G Loss: 1.5995\n",
      "Epoch 118 - D Loss: -0.0020, G Loss: 1.5742\n",
      "Mean L1 Loss: 1.227705, Mean L2 Loss: 3.218161\n",
      "Epoch 119, Iter 4650, D Loss: -0.0031, G Loss: 1.5570, L1: 1.2277, L2: 3.2182\n",
      "Epoch 119 - D Loss: 0.0025, G Loss: 1.5877\n",
      "Mean L1 Loss: 1.214831, Mean L2 Loss: 3.223413\n",
      "Epoch 120, Iter 4700, D Loss: -0.0033, G Loss: 1.6009, L1: 1.2148, L2: 3.2234\n",
      "Epoch 120 - D Loss: 0.0034, G Loss: 1.5870\n",
      "Mean L1 Loss: 1.245344, Mean L2 Loss: 3.287493\n",
      "Epoch 121, Iter 4750, D Loss: 0.0039, G Loss: 1.5424, L1: 1.2453, L2: 3.2875\n",
      "Epoch 121 - D Loss: 0.0030, G Loss: 1.5671\n",
      "Epoch 122 - D Loss: -0.0006, G Loss: 1.6245\n",
      "Mean L1 Loss: 1.234043, Mean L2 Loss: 3.245674\n",
      "Epoch 123, Iter 4800, D Loss: 0.0004, G Loss: 1.5775, L1: 1.2340, L2: 3.2457\n",
      "Epoch 123 - D Loss: -0.0021, G Loss: 1.5845\n",
      "Mean L1 Loss: 1.219580, Mean L2 Loss: 3.192975\n",
      "Epoch 124, Iter 4850, D Loss: 0.0051, G Loss: 1.5988, L1: 1.2196, L2: 3.1930\n",
      "Epoch 124 - D Loss: 0.0026, G Loss: 1.5886\n",
      "Mean L1 Loss: 1.241648, Mean L2 Loss: 3.321429\n",
      "Epoch 125, Iter 4900, D Loss: 0.0020, G Loss: 1.5640, L1: 1.2416, L2: 3.3214\n",
      "Epoch 125 - D Loss: -0.0016, G Loss: 1.5631\n",
      "Mean L1 Loss: 1.225650, Mean L2 Loss: 3.225194\n",
      "Epoch 126, Iter 4950, D Loss: 0.0022, G Loss: 1.5795, L1: 1.2256, L2: 3.2252\n",
      "Epoch 126 - D Loss: 0.0017, G Loss: 1.5748\n",
      "Epoch 127 - D Loss: -0.0027, G Loss: 1.5651\n",
      "Mean L1 Loss: 1.221066, Mean L2 Loss: 3.204730\n",
      "Epoch 128, Iter 5000, D Loss: 0.0093, G Loss: 1.6098, L1: 1.2211, L2: 3.2047\n",
      "Epoch 128 - D Loss: 0.0020, G Loss: 1.6062\n",
      "Mean L1 Loss: 1.236856, Mean L2 Loss: 3.248996\n",
      "Epoch 129, Iter 5050, D Loss: -0.0006, G Loss: 1.5841, L1: 1.2369, L2: 3.2490\n",
      "Epoch 129 - D Loss: 0.0027, G Loss: 1.5616\n",
      "Mean L1 Loss: 1.217344, Mean L2 Loss: 3.193789\n",
      "Epoch 130, Iter 5100, D Loss: 0.0022, G Loss: 1.5658, L1: 1.2173, L2: 3.1938\n",
      "Epoch 130 - D Loss: -0.0006, G Loss: 1.5823\n",
      "Epoch 131 - D Loss: -0.0022, G Loss: 1.5406\n",
      "Mean L1 Loss: 1.241430, Mean L2 Loss: 3.262846\n",
      "Epoch 132, Iter 5150, D Loss: 0.0088, G Loss: 1.5380, L1: 1.2414, L2: 3.2628\n",
      "Epoch 132 - D Loss: -0.0025, G Loss: 1.5545\n",
      "Mean L1 Loss: 1.227772, Mean L2 Loss: 3.184445\n",
      "Epoch 133, Iter 5200, D Loss: -0.0065, G Loss: 1.5594, L1: 1.2278, L2: 3.1844\n",
      "Epoch 133 - D Loss: 0.0002, G Loss: 1.5700\n",
      "Mean L1 Loss: 1.297803, Mean L2 Loss: 3.464621\n",
      "Epoch 134, Iter 5250, D Loss: 0.0008, G Loss: 1.5607, L1: 1.2978, L2: 3.4646\n",
      "Epoch 134 - D Loss: -0.0009, G Loss: 1.5628\n",
      "Mean L1 Loss: 1.224821, Mean L2 Loss: 3.240884\n",
      "Epoch 135, Iter 5300, D Loss: 0.0023, G Loss: 1.6064, L1: 1.2248, L2: 3.2409\n",
      "Epoch 135 - D Loss: 0.0022, G Loss: 1.6118\n",
      "Epoch 136 - D Loss: 0.0021, G Loss: 1.5344\n",
      "Mean L1 Loss: 1.263155, Mean L2 Loss: 3.342213\n",
      "Epoch 137, Iter 5350, D Loss: -0.0057, G Loss: 1.5212, L1: 1.2632, L2: 3.3422\n",
      "Epoch 137 - D Loss: -0.0002, G Loss: 1.5422\n",
      "Mean L1 Loss: 1.243627, Mean L2 Loss: 3.302210\n",
      "Epoch 138, Iter 5400, D Loss: 0.0037, G Loss: 1.6397, L1: 1.2436, L2: 3.3022\n",
      "Epoch 138 - D Loss: 0.0002, G Loss: 1.6082\n",
      "Mean L1 Loss: 1.224564, Mean L2 Loss: 3.236149\n",
      "Epoch 139, Iter 5450, D Loss: 0.0041, G Loss: 1.6341, L1: 1.2246, L2: 3.2361\n",
      "Epoch 139 - D Loss: 0.0031, G Loss: 1.6035\n",
      "Epoch 140 - D Loss: 0.0029, G Loss: 1.6103\n",
      "Mean L1 Loss: 1.230383, Mean L2 Loss: 3.260776\n",
      "Epoch 141, Iter 5500, D Loss: -0.0218, G Loss: 1.6791, L1: 1.2304, L2: 3.2608\n",
      "Epoch 141 - D Loss: 0.0028, G Loss: 1.5890\n",
      "Mean L1 Loss: 1.248516, Mean L2 Loss: 3.247534\n",
      "Epoch 142, Iter 5550, D Loss: 0.0042, G Loss: 1.6617, L1: 1.2485, L2: 3.2475\n",
      "Epoch 142 - D Loss: -0.0004, G Loss: 1.6050\n",
      "Mean L1 Loss: 1.223064, Mean L2 Loss: 3.225949\n",
      "Epoch 143, Iter 5600, D Loss: -0.0016, G Loss: 1.5867, L1: 1.2231, L2: 3.2259\n",
      "Epoch 143 - D Loss: -0.0017, G Loss: 1.6107\n",
      "Mean L1 Loss: 1.240222, Mean L2 Loss: 3.261257\n",
      "Epoch 144, Iter 5650, D Loss: 0.0041, G Loss: 1.6034, L1: 1.2402, L2: 3.2613\n",
      "Epoch 144 - D Loss: 0.0058, G Loss: 1.5961\n",
      "Epoch 145 - D Loss: 0.0013, G Loss: 1.6231\n",
      "Mean L1 Loss: 1.226782, Mean L2 Loss: 3.207741\n",
      "Epoch 146, Iter 5700, D Loss: 0.0043, G Loss: 1.6077, L1: 1.2268, L2: 3.2077\n",
      "Epoch 146 - D Loss: -0.0007, G Loss: 1.6332\n",
      "Mean L1 Loss: 1.216798, Mean L2 Loss: 3.166276\n",
      "Epoch 147, Iter 5750, D Loss: 0.0057, G Loss: 1.6769, L1: 1.2168, L2: 3.1663\n",
      "Epoch 147 - D Loss: 0.0026, G Loss: 1.6245\n",
      "Mean L1 Loss: 1.216286, Mean L2 Loss: 3.176593\n",
      "Epoch 148, Iter 5800, D Loss: 0.0011, G Loss: 1.6192, L1: 1.2163, L2: 3.1766\n",
      "Epoch 148 - D Loss: -0.0002, G Loss: 1.6228\n",
      "Mean L1 Loss: 1.230133, Mean L2 Loss: 3.236793\n",
      "Epoch 149, Iter 5850, D Loss: 0.0030, G Loss: 1.6330, L1: 1.2301, L2: 3.2368\n",
      "Epoch 149 - D Loss: 0.0030, G Loss: 1.6330\n",
      "Epoch 150 - D Loss: 0.0027, G Loss: 1.6403\n",
      "Mean L1 Loss: 1.254476, Mean L2 Loss: 3.286238\n",
      "Epoch 151, Iter 5900, D Loss: 0.0015, G Loss: 1.5851, L1: 1.2545, L2: 3.2862\n",
      "Epoch 151 - D Loss: -0.0008, G Loss: 1.6319\n",
      "Mean L1 Loss: 1.218308, Mean L2 Loss: 3.199964\n",
      "Epoch 152, Iter 5950, D Loss: 0.0106, G Loss: 1.5945, L1: 1.2183, L2: 3.2000\n",
      "Epoch 152 - D Loss: 0.0062, G Loss: 1.5989\n",
      "Mean L1 Loss: 1.222439, Mean L2 Loss: 3.226456\n",
      "Epoch 153, Iter 6000, D Loss: -0.0005, G Loss: 1.6475, L1: 1.2224, L2: 3.2265\n",
      "Epoch 153 - D Loss: -0.0016, G Loss: 1.6504\n",
      "Epoch 154 - D Loss: 0.0059, G Loss: 1.6440\n",
      "Mean L1 Loss: 1.229500, Mean L2 Loss: 3.258326\n",
      "Epoch 155, Iter 6050, D Loss: 0.0118, G Loss: 1.6487, L1: 1.2295, L2: 3.2583\n",
      "Epoch 155 - D Loss: 0.0016, G Loss: 1.6720\n",
      "Mean L1 Loss: 1.224103, Mean L2 Loss: 3.258958\n",
      "Epoch 156, Iter 6100, D Loss: -0.0036, G Loss: 1.6774, L1: 1.2241, L2: 3.2590\n",
      "Epoch 156 - D Loss: 0.0032, G Loss: 1.6692\n",
      "Mean L1 Loss: 1.231274, Mean L2 Loss: 3.280646\n",
      "Epoch 157, Iter 6150, D Loss: 0.0010, G Loss: 1.6291, L1: 1.2313, L2: 3.2806\n",
      "Epoch 157 - D Loss: 0.0021, G Loss: 1.6237\n",
      "Mean L1 Loss: 1.212188, Mean L2 Loss: 3.145226\n",
      "Epoch 158, Iter 6200, D Loss: 0.0013, G Loss: 1.6155, L1: 1.2122, L2: 3.1452\n",
      "Epoch 158 - D Loss: 0.0008, G Loss: 1.6211\n",
      "Epoch 159 - D Loss: 0.0052, G Loss: 1.6693\n",
      "Mean L1 Loss: 1.221696, Mean L2 Loss: 3.201440\n",
      "Epoch 160, Iter 6250, D Loss: 0.0036, G Loss: 1.6638, L1: 1.2217, L2: 3.2014\n",
      "Epoch 160 - D Loss: 0.0037, G Loss: 1.6683\n",
      "Mean L1 Loss: 1.215230, Mean L2 Loss: 3.185106\n",
      "Epoch 161, Iter 6300, D Loss: 0.0015, G Loss: 1.6748, L1: 1.2152, L2: 3.1851\n",
      "Epoch 161 - D Loss: 0.0023, G Loss: 1.6731\n",
      "Mean L1 Loss: 1.226573, Mean L2 Loss: 3.243948\n",
      "Epoch 162, Iter 6350, D Loss: 0.0008, G Loss: 1.6676, L1: 1.2266, L2: 3.2439\n",
      "Epoch 162 - D Loss: 0.0014, G Loss: 1.6726\n",
      "Epoch 163 - D Loss: 0.0037, G Loss: 1.6474\n",
      "Mean L1 Loss: 1.224345, Mean L2 Loss: 3.197487\n",
      "Epoch 164, Iter 6400, D Loss: -0.0010, G Loss: 1.7570, L1: 1.2243, L2: 3.1975\n",
      "Epoch 164 - D Loss: 0.0029, G Loss: 1.7208\n",
      "Mean L1 Loss: 1.250743, Mean L2 Loss: 3.325146\n",
      "Epoch 165, Iter 6450, D Loss: 0.0053, G Loss: 1.6242, L1: 1.2507, L2: 3.3251\n",
      "Epoch 165 - D Loss: 0.0042, G Loss: 1.6688\n",
      "Mean L1 Loss: 1.256916, Mean L2 Loss: 3.319144\n",
      "Epoch 166, Iter 6500, D Loss: 0.0025, G Loss: 1.6776, L1: 1.2569, L2: 3.3191\n",
      "Epoch 166 - D Loss: 0.0036, G Loss: 1.6974\n",
      "Mean L1 Loss: 1.213804, Mean L2 Loss: 3.142756\n",
      "Epoch 167, Iter 6550, D Loss: 0.0045, G Loss: 1.7056, L1: 1.2138, L2: 3.1428\n",
      "Epoch 167 - D Loss: 0.0039, G Loss: 1.7130\n",
      "Epoch 168 - D Loss: 0.0030, G Loss: 1.6864\n",
      "Mean L1 Loss: 1.241328, Mean L2 Loss: 3.315226\n",
      "Epoch 169, Iter 6600, D Loss: -0.0062, G Loss: 1.6636, L1: 1.2413, L2: 3.3152\n",
      "Epoch 169 - D Loss: -0.0004, G Loss: 1.7077\n",
      "Mean L1 Loss: 1.221263, Mean L2 Loss: 3.222479\n",
      "Epoch 170, Iter 6650, D Loss: 0.0065, G Loss: 1.7692, L1: 1.2213, L2: 3.2225\n",
      "Epoch 170 - D Loss: 0.0032, G Loss: 1.7483\n",
      "Mean L1 Loss: 1.213459, Mean L2 Loss: 3.138246\n",
      "Epoch 171, Iter 6700, D Loss: 0.0005, G Loss: 1.7382, L1: 1.2135, L2: 3.1382\n",
      "Saved best model with L2: 3.1382\n",
      "Epoch 171 - D Loss: -0.0007, G Loss: 1.7344\n",
      "Epoch 172 - D Loss: 0.0020, G Loss: 1.7746\n",
      "Mean L1 Loss: 1.272008, Mean L2 Loss: 3.347081\n",
      "Epoch 173, Iter 6750, D Loss: -0.0004, G Loss: 1.8557, L1: 1.2720, L2: 3.3471\n",
      "Epoch 173 - D Loss: 0.0025, G Loss: 1.7591\n",
      "Mean L1 Loss: 1.233724, Mean L2 Loss: 3.298225\n",
      "Epoch 174, Iter 6800, D Loss: 0.0065, G Loss: 1.7167, L1: 1.2337, L2: 3.2982\n",
      "Epoch 174 - D Loss: 0.0028, G Loss: 1.7275\n",
      "Mean L1 Loss: 1.229172, Mean L2 Loss: 3.207609\n",
      "Epoch 175, Iter 6850, D Loss: 0.0049, G Loss: 1.7845, L1: 1.2292, L2: 3.2076\n",
      "Epoch 175 - D Loss: 0.0070, G Loss: 1.7647\n",
      "Mean L1 Loss: 1.223695, Mean L2 Loss: 3.231375\n",
      "Epoch 176, Iter 6900, D Loss: 0.0024, G Loss: 1.7097, L1: 1.2237, L2: 3.2314\n",
      "Epoch 176 - D Loss: 0.0022, G Loss: 1.7082\n",
      "Epoch 177 - D Loss: 0.0029, G Loss: 1.7417\n",
      "Mean L1 Loss: 1.220980, Mean L2 Loss: 3.190398\n",
      "Epoch 178, Iter 6950, D Loss: -0.0076, G Loss: 1.7142, L1: 1.2210, L2: 3.1904\n",
      "Epoch 178 - D Loss: 0.0023, G Loss: 1.7064\n",
      "Mean L1 Loss: 1.237880, Mean L2 Loss: 3.249766\n",
      "Epoch 179, Iter 7000, D Loss: -0.0024, G Loss: 1.6412, L1: 1.2379, L2: 3.2498\n",
      "Epoch 179 - D Loss: 0.0011, G Loss: 1.6891\n",
      "Mean L1 Loss: 1.219759, Mean L2 Loss: 3.178157\n",
      "Epoch 180, Iter 7050, D Loss: 0.0020, G Loss: 1.6615, L1: 1.2198, L2: 3.1782\n",
      "Epoch 180 - D Loss: 0.0067, G Loss: 1.6686\n",
      "Epoch 181 - D Loss: 0.0016, G Loss: 1.7016\n",
      "Mean L1 Loss: 1.228645, Mean L2 Loss: 3.232288\n",
      "Epoch 182, Iter 7100, D Loss: -0.0086, G Loss: 1.6117, L1: 1.2286, L2: 3.2323\n",
      "Epoch 182 - D Loss: 0.0040, G Loss: 1.6719\n",
      "Mean L1 Loss: 1.237590, Mean L2 Loss: 3.287457\n",
      "Epoch 183, Iter 7150, D Loss: -0.0010, G Loss: 1.6461, L1: 1.2376, L2: 3.2875\n",
      "Epoch 183 - D Loss: 0.0030, G Loss: 1.6908\n",
      "Mean L1 Loss: 1.236593, Mean L2 Loss: 3.288409\n",
      "Epoch 184, Iter 7200, D Loss: 0.0035, G Loss: 1.7231, L1: 1.2366, L2: 3.2884\n",
      "Epoch 184 - D Loss: 0.0022, G Loss: 1.7179\n",
      "Mean L1 Loss: 1.249219, Mean L2 Loss: 3.317605\n",
      "Epoch 185, Iter 7250, D Loss: -0.0016, G Loss: 1.6056, L1: 1.2492, L2: 3.3176\n",
      "Epoch 185 - D Loss: -0.0004, G Loss: 1.5992\n",
      "Epoch 186 - D Loss: 0.0047, G Loss: 1.7024\n",
      "Mean L1 Loss: 1.245642, Mean L2 Loss: 3.311247\n",
      "Epoch 187, Iter 7300, D Loss: -0.0018, G Loss: 1.7525, L1: 1.2456, L2: 3.3112\n",
      "Epoch 187 - D Loss: 0.0023, G Loss: 1.7017\n",
      "Mean L1 Loss: 1.228062, Mean L2 Loss: 3.288315\n",
      "Epoch 188, Iter 7350, D Loss: -0.0006, G Loss: 1.6915, L1: 1.2281, L2: 3.2883\n",
      "Epoch 188 - D Loss: 0.0021, G Loss: 1.6746\n",
      "Mean L1 Loss: 1.236999, Mean L2 Loss: 3.311477\n",
      "Epoch 189, Iter 7400, D Loss: 0.0053, G Loss: 1.7139, L1: 1.2370, L2: 3.3115\n",
      "Epoch 189 - D Loss: 0.0023, G Loss: 1.7231\n",
      "Epoch 190 - D Loss: -0.0012, G Loss: 1.6605\n",
      "Mean L1 Loss: 1.220007, Mean L2 Loss: 3.201647\n",
      "Epoch 191, Iter 7450, D Loss: 0.0164, G Loss: 1.5729, L1: 1.2200, L2: 3.2016\n",
      "Epoch 191 - D Loss: 0.0063, G Loss: 1.7086\n",
      "Mean L1 Loss: 1.227210, Mean L2 Loss: 3.210180\n",
      "Epoch 192, Iter 7500, D Loss: 0.0059, G Loss: 1.7113, L1: 1.2272, L2: 3.2102\n",
      "Epoch 192 - D Loss: 0.0060, G Loss: 1.7137\n",
      "Mean L1 Loss: 1.222258, Mean L2 Loss: 3.234535\n",
      "Epoch 193, Iter 7550, D Loss: -0.0030, G Loss: 1.7260, L1: 1.2223, L2: 3.2345\n",
      "Epoch 193 - D Loss: 0.0009, G Loss: 1.7200\n",
      "Mean L1 Loss: 1.227387, Mean L2 Loss: 3.275983\n",
      "Epoch 194, Iter 7600, D Loss: 0.0073, G Loss: 1.6783, L1: 1.2274, L2: 3.2760\n",
      "Epoch 194 - D Loss: 0.0073, G Loss: 1.6727\n",
      "Epoch 195 - D Loss: 0.0037, G Loss: 1.6668\n",
      "Mean L1 Loss: 1.225451, Mean L2 Loss: 3.257945\n",
      "Epoch 196, Iter 7650, D Loss: 0.0208, G Loss: 1.7542, L1: 1.2255, L2: 3.2579\n",
      "Epoch 196 - D Loss: 0.0042, G Loss: 1.6994\n",
      "Mean L1 Loss: 1.223200, Mean L2 Loss: 3.194496\n",
      "Epoch 197, Iter 7700, D Loss: 0.0047, G Loss: 1.6441, L1: 1.2232, L2: 3.1945\n",
      "Epoch 197 - D Loss: 0.0028, G Loss: 1.6468\n",
      "Mean L1 Loss: 1.235139, Mean L2 Loss: 3.291010\n",
      "Epoch 198, Iter 7750, D Loss: 0.0081, G Loss: 1.6450, L1: 1.2351, L2: 3.2910\n",
      "Epoch 198 - D Loss: 0.0057, G Loss: 1.6471\n",
      "Mean L1 Loss: 1.211635, Mean L2 Loss: 3.147231\n",
      "Epoch 199, Iter 7800, D Loss: -0.0002, G Loss: 1.6659, L1: 1.2116, L2: 3.1472\n",
      "Epoch 199 - D Loss: -0.0002, G Loss: 1.6659\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "trained_G, trained_D = train_WGR_fnn(D=D_net, G=G_net, D_solver=D_solver, G_solver=G_solver, \n",
    "                                     loader_train = loader_train, loader_val=loader_val,\n",
    "                                     noise_dim=args.noise_dim, Xdim=args.Xdim, Ydim=args.Ydim, batch_size=args.train_batch,\n",
    "                                     save_path='./', device=device, num_epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "daf00ecd-57df-4b58-841e-932233afcf78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: M1, Univariate, Ydim: 1, J_t_size: 50\n",
      "L1 Loss: tensor([1.1977])\n",
      "L2 Loss: tensor([2.9525])\n",
      "MSE Mean: tensor([0.2268])\n",
      "MSE SD: tensor([0.2009])\n"
     ]
    }
   ],
   "source": [
    "# Calculate the L1 and L2 error, MSE of conditional mean and conditional standard deviation on the test data  \n",
    "test_G_mean_sd = L1L2_MSE_mean_sd_G(G = trained_G.to('cpu'),  test_size = args.test, noise_dim=args.noise_dim, Xdim=args.Xdim, \n",
    "                                    batch_size=args.test_batch, loader_dataset = loader_test )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a53a40ac-b651-443f-8476-bbc9949dbeca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q_5: 0.5895, Q_25: 0.3080, Q_50: 0.2314, Q_75: 0.4557, Q_95: 0.9007\n"
     ]
    }
   ],
   "source": [
    "# Calculate the MSE of conditional quantiles at different levels.\n",
    "test_G_quantile = MSE_quantile_G_uniY(G = trained_G.to('cpu'), loader_dataset = loader_test , noise_dim=args.noise_dim, Xdim=args.Xdim, \n",
    "                                      test_size = args.test,  batch_size=args.test_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc16ad8-4d13-4b6f-ac05-8c45ab75c6e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
